{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKSFbiqpryWM2Of5pA1IOZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkStephens060482/Big-Data-projects/blob/main/Concurrent_API_calls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concurrent Historical Weather API requests\n",
        "Individual historical weather observations based on Latitude and Longitude coordinate location and datetime incident.\n",
        "\n",
        "1. Dataframe with LATITUDE, LONGITUDE, DATE and TIME features is split into batches of specified **batch_size**.\n",
        "2. api keys are rotated amoungst each batch.\n",
        "3. one at a time each batch is passed to the  **get_weather_in_batches()** function, under a max concurrency limit.\n",
        "4. The program then loops through each row in the batch and an API query is built based on the feature values.\n",
        "5. The API call is performed. If any of the API calls failure due to the API service system constraints, the whole batch is tried again up to the specified **max_retries**.\n",
        "6. The results from the API call are converted to json and added to a list.\n",
        "7. This is performed asynchronously for all batches and the results are gathered and awaited upon until all batches are completed.\n",
        "8. All API results in JSON in each batch are flattened into a single list.\n",
        "9. The complete list is converted to DataFrame and is joined to the original DataFrame in correct order, and is written to CSV."
      ],
      "metadata": {
        "id": "-g2PuPmWIaZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTODBDIkHs53"
      },
      "outputs": [],
      "source": [
        "import math, time\n",
        "import pandas as pd\n",
        "import csv\n",
        "import httpx\n",
        "import asyncio\n",
        "import sys\n",
        "import json\n",
        "import datetime as dt\n",
        "\n",
        "###API Service###\n",
        "'https://www.visualcrossing.com/weather-api'\n",
        "\n",
        "# Define global configuration variables\n",
        "config = {\n",
        "    'max_concurrent_batches': 5, # Adjust the value based on fair use limits\n",
        "     #'time_period_minutes': 10,  # Adjust the value based on fair use limits\n",
        "    'batch_size': 200, # adjust based on system performance\n",
        "    'max_retries': 3,\n",
        "    'api_keys': ['api_key_1', 'api_key_2', 'api_key_3'],  # enter your own API keys from service subscription account(s).\n",
        "    'max_rate_limit':20 # requests per second\n",
        "}\n",
        "\n",
        "#define the fuction to perform api calls to get historical weather data.\n",
        "async def get_weather_in_batches(batch, api_key,max_retries):\n",
        "    '''\n",
        "    '''\n",
        "    async with httpx.AsyncClient() as client:\n",
        "      for retry in range(max_retries + 1):\n",
        "        try:\n",
        "          batch_results = []\n",
        "          for row in batch.to_dict(orient=\"records\"):\n",
        "            # Create the query parameters based on row features and the API key\n",
        "            ApiQuery = create_query(row)\n",
        "            #query parameters\n",
        "            url_params = {'unitGroup':'metric', #UnitGroup sets the units of the output - SI metrics\n",
        "                          'contentType':'json',\n",
        "                          'include':'current', #Include current conditions of weather\n",
        "                          'maxDistance':'25000',#Interpolate weather from station within max distance of 2km\n",
        "                          'maxStations':'3', #Interpolate weather from 3 stations within max distance.\n",
        "                          #'allowAsynch':'true', # allow synchronous requests\n",
        "                          'aggregateHours':'0',\n",
        "                          'aggregateMinutes':'15',\n",
        "                          'combinationMethod':'aggregate',\n",
        "                          'key':api_key\n",
        "                          }\n",
        "            # Send the API request with query parameters\n",
        "            response = await client.get(ApiQuery, params = url_params)\n",
        "            # Raise an exception if the response status code is an error\n",
        "            response.raise_for_status()\n",
        "            # Convert the response to JSON\n",
        "            json_weatherdata = response.json()\n",
        "            #add result to batch\n",
        "            batch_results.append(json_weatherdata['currentConditions'])\n",
        "            # Calculate the delay based on the maximum rate limit\n",
        "            delay = 1 / config['max_rate_limit']\n",
        "            # Introduce the delay before making the next request\n",
        "            await asyncio.sleep(delay)\n",
        "\n",
        "          return batch_results\n",
        "        #error handling\n",
        "        except httpx.HTTPError as e:\n",
        "          if e.response.status_code == 429:\n",
        "            print(\"Fair use limit exceeded. Retrying the batch...\")\n",
        "            #Retry the entire batch\n",
        "            continue\n",
        "          else:\n",
        "            print(f\"HTTP error occurred: {e}\")\n",
        "            break  # Break the retry loop for other HTTP errors\n",
        "        except Exception as e:\n",
        "              print(f\"Exception occurred: {e}\")\n",
        "              break  # Break the retry loop for other errors\n",
        "\n",
        "    # Return empty results if all retries failed\n",
        "    return[]\n",
        "\n",
        "# Create the query parameters based on row features.\n",
        "def create_query(row):\n",
        "  '''\n",
        "  '''\n",
        "  #API base url\n",
        "  BaseURL = 'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/'\n",
        "  # Modify this function based on the structure of your API and DataFrame\n",
        "  Location = f\"{row['LATITUDE']},{row['LONGITUDE']}\"\n",
        "  date = row['ACCIDENT_DATE']\n",
        "  time = row['ACCIDENT_TIME']\n",
        "  startDateTime = f\"{date}T{time}\"\n",
        "  #basic query including location\n",
        "  ApiQuery = f\"{BaseURL}{Location}/{startDateTime}\"\n",
        "  return ApiQuery\n",
        "\n",
        "def split_df_chunks(data_df, chunk_size):\n",
        "    total_length = len(data_df)\n",
        "    total_chunk_num = math.ceil(total_length / chunk_size)\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, total_length, chunk_size):\n",
        "        chunk = data_df[i : i + chunk_size]\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Function to process the DataFrame rows asynchronously\n",
        "async def process_batches(data_df, api_keys, max_retries, batch_size):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  total_length = len(data_df)\n",
        "  semaphore = asyncio.Semaphore(config['max_concurrent_batches'])\n",
        "  tasks = []\n",
        "\n",
        "  # split df into batches\n",
        "  batches = split_df_chunks(data_df,batch_size)\n",
        "\n",
        "  request_number = 0\n",
        "  elapsed_time = 0\n",
        "  for batch_count,batch in enumerate(batches):\n",
        "\n",
        "    # sequentially assign an API key based on the batch\n",
        "    api_key = api_keys[batch_count % len(api_keys)]\n",
        "    # Start time of batch processing\n",
        "    start_time = time.perf_counter()\n",
        "    # Perform chunk API call in loop with retry batch method\n",
        "    async with semaphore:\n",
        "          # perform batch API tasks in loop\n",
        "          results = asyncio.create_task(get_weather_in_batches(batch, api_key,max_retries))\n",
        "          tasks.append(results)\n",
        "\n",
        "          request_number += len(batch)\n",
        "          # Print the progress of successfully processed batches\n",
        "          print(f\"Processed {request_number*100/total_length: 0.2f}% of total requests successfully.\")\n",
        "    # End time of batch processing\n",
        "    end_time = time.perf_counter()\n",
        "    elapsed_time += (end_time - start_time)\n",
        "    print(f\"Total time taken to process batches: {elapsed_time: 0.4f} seconds\")\n",
        "    # Wait for all tasks to complete\n",
        "    results = await asyncio.gather(*tasks,return_exceptions=True)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "### Main Code ###\n",
        "async def main(df):\n",
        "  start_time = time.perf_counter()\n",
        "  # Perform asynchronous processing on each partition in batches\n",
        "  results = await process_batches(df,\n",
        "                                  api_keys = config['api_keys'],\n",
        "                                  max_retries = config['max_retries'],\n",
        "                                  batch_size = config['batch_size'])\n",
        "\n",
        "  # Convert the results to Row objects with the defined schema\n",
        "  Flatten_results = [result for batch_results in results for result in batch_results]\n",
        "\n",
        "  # Create DataFrame using the rows with schema\n",
        "  df_with_results = pd.DataFrame(Flatten_results)\n",
        "\n",
        "  # Append the new DataFrame to the original DataFrame\n",
        "  output_df = pd.concat([df, df_with_results], axis=1,copy = True)\n",
        "  end_time =  time.perf_counter()\n",
        "  time_duration = end_time - start_time\n",
        "  print(f'Total computation time is : {time_duration: 0.1f} seconds')\n",
        "  print(f'Average computation time is : {time_duration/len(df_with_results): 0.1f} seconds per row')\n",
        "  return output_df\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  output_df = await main(weather_df)\n",
        ""
      ]
    }
  ]
}